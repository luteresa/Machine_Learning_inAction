{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 条件概率\n",
    "\\begin{equation}\n",
    "p(c|x)= \\frac{p(x|c)*p(c)}{p(x)}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#创建数据\n",
    "def loadDataSet():\n",
    "    postingList=[['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],\n",
    "                 ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],\n",
    "                 ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],\n",
    "                 ['stop', 'posting', 'stupid', 'worthless', 'garbage'],\n",
    "                 ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],\n",
    "                 ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]\n",
    "    classVec = [0,1,0,1,0,1]    #1 is abusive, 0 not\n",
    "    return postingList,classVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'], ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'], ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'], ['stop', 'posting', 'stupid', 'worthless', 'garbage'], ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'], ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]\n"
     ]
    }
   ],
   "source": [
    "listOfPosts,listClasses = loadDataSet()\n",
    "print(listOfPosts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#创建数据集的词汇表集合(无重复词汇)\n",
    "def createVocabList(dataSet):\n",
    "    vocabSet = set([])\n",
    "    for document in dataSet:\n",
    "        vocabSet = vocabSet | set(document)\n",
    "        #print(vocabSet)\n",
    "    return list(vocabSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['worthless', 'stop', 'take', 'problems', 'my', 'has', 'help', 'quit', 'love', 'dalmation', 'please', 'steak', 'posting', 'is', 'I', 'park', 'dog', 'garbage', 'food', 'maybe', 'to', 'buying', 'not', 'stupid', 'ate', 'how', 'cute', 'so', 'him', 'mr', 'licks', 'flea']\n"
     ]
    }
   ],
   "source": [
    "myVocabList = createVocabList(listOfPosts)\n",
    "print(myVocabList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "func:词汇集合转换为向量\n",
    "vocabList: 训练集的词汇集合\n",
    "inputSet: 待测试的文本\n",
    "\n",
    "return: 待测文本，相对训练集的向量\n",
    "'''\n",
    "def setOfWord2Vec(vocabList, inputSet):\n",
    "    returnVec = [0]*len(vocabList)\n",
    "    #print(returnVec)\n",
    "    for word in inputSet:\n",
    "        if word in vocabList:\n",
    "            returnVec[vocabList.index(word)] = 1\n",
    "        else:\n",
    "            print(\"the word:{} is not in my Vocabulary\".format(word))\n",
    "    \n",
    "    return returnVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "a=setOfWord2Vec(myVocabList,listOfPosts[0])\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "func:朴素贝叶斯算法训练\n",
    "\n",
    "trainMatrix: 训练数据的向量集\n",
    "trainCategory: 训练数据的分类向量\n",
    "\n",
    "return: \n",
    "p1Vect：标签1条件下，特征向量，各个特征值的条件概率\n",
    "p0Vect：标签0条件下，特征向量，各个特征值的条件概率\n",
    "pAbusive：标签1的概率\n",
    "'''\n",
    "def trainNB0(trainMatrix, trainCategory):\n",
    "    numTrainDocs = len(trainMatrix)\n",
    "    numWord = len(trainMatrix[0])\n",
    "    pAbusive = sum(trainCategory)/float(numTrainDocs)\n",
    "    p0Num = np.ones(numWord);\n",
    "    p1Num = np.ones(numWord)\n",
    "    #print('p0Num',p0Num)\n",
    "    p0Denom = 2.\n",
    "    p1Denom = 2\n",
    "    \n",
    "    for i in range(numTrainDocs):\n",
    "        if trainCategory[i] == 1:\n",
    "            p1Num += trainMatrix[i]\n",
    "            p1Denom += sum(trainMatrix[i])\n",
    "        else:\n",
    "            p0Num += trainMatrix[i]\n",
    "            p0Denom += sum(trainMatrix[i])\n",
    "    p1Vect = np.log(p1Num/p1Denom)\n",
    "    p0Vect = np.log(p0Num/p0Denom)\n",
    "    \n",
    "    return p0Vect,p1Vect,pAbusive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "listOfPosts,listClasses = loadDataSet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['worthless', 'stop', 'take', 'problems', 'my', 'has', 'help', 'quit', 'love', 'dalmation', 'please', 'steak', 'posting', 'is', 'I', 'park', 'dog', 'garbage', 'food', 'maybe', 'to', 'buying', 'not', 'stupid', 'ate', 'how', 'cute', 'so', 'him', 'mr', 'licks', 'flea']\n"
     ]
    }
   ],
   "source": [
    "myVocabList = createVocabList(listOfPosts)\n",
    "print(myVocabList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0], [0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0], [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0], [1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']\n"
     ]
    }
   ],
   "source": [
    "trainMat = []\n",
    "for postinDoc in listOfPosts:\n",
    "    #print(postinDoc)\n",
    "    trainMat.append(setOfWord2Vec(myVocabList,postinDoc))\n",
    "print(trainMat)\n",
    "print(listOfPosts[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "p0V,p1V,pAb = trainNB0(trainMat,listClasses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pAb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3.25809654, -2.56494936, -3.25809654, -2.56494936, -1.87180218,\n",
       "       -2.56494936, -2.56494936, -3.25809654, -2.56494936, -2.56494936,\n",
       "       -2.56494936, -2.56494936, -3.25809654, -2.56494936, -2.56494936,\n",
       "       -3.25809654, -2.56494936, -3.25809654, -3.25809654, -3.25809654,\n",
       "       -2.56494936, -3.25809654, -3.25809654, -3.25809654, -2.56494936,\n",
       "       -2.56494936, -2.56494936, -2.56494936, -2.15948425, -2.56494936,\n",
       "       -2.56494936, -2.56494936])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p0V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.94591015, -2.35137526, -2.35137526, -3.04452244, -3.04452244,\n",
       "       -3.04452244, -3.04452244, -2.35137526, -3.04452244, -3.04452244,\n",
       "       -3.04452244, -3.04452244, -2.35137526, -3.04452244, -3.04452244,\n",
       "       -2.35137526, -1.94591015, -2.35137526, -2.35137526, -2.35137526,\n",
       "       -2.35137526, -2.35137526, -2.35137526, -1.65822808, -3.04452244,\n",
       "       -3.04452244, -3.04452244, -3.04452244, -2.35137526, -3.04452244,\n",
       "       -3.04452244, -3.04452244])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p1V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "func: 分类器\n",
    "\n",
    "输入：待测向量，特征值向量的条件概率，各标签类的概率\n",
    "\n",
    "return: 待测随机变量与各类标签类联合概率分布的最大值；\n",
    "\n",
    "'''\n",
    "def classifyNB(vec2Classify,p0Vec,p1Vec,pClass1):\n",
    "    p1 = np.sum(vec2Classify*p1Vec) + np.log(pClass1)\n",
    "    p0 = np.sum(vec2Classify*p0Vec) + np.log(1.0-pClass1)\n",
    "    if p1 > p0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testingNB():\n",
    "    listOfPosts,listClasses = loadDataSet()\n",
    "    myVocabList = createVocabList(listOfPosts)\n",
    "    trainMat=[]\n",
    "    for postinDoc in listOfPosts:\n",
    "        trainMat.append(setOfWord2Vec(myVocabList,postinDoc))\n",
    "    p0V,p1V,pAb = trainNB0(np.array(trainMat),np.array(listClasses))\n",
    "    \n",
    "    testEntry = ['stupid','garbage']\n",
    "    testEntry1=['hello','good']\n",
    "    #thisDoc:文档testEntry的特征向量\n",
    "    thisDoc = np.array(setOfWord2Vec(myVocabList,testEntry1))\n",
    "    print(\"{}clasified as:{}\".format(testEntry1,classifyNB(thisDoc,p0V,p1V,pAb)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the word:hello is not in my Vocabulary\n",
      "the word:good is not in my Vocabulary\n",
      "['hello', 'good']clasified as:0\n"
     ]
    }
   ],
   "source": [
    "testingNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "mySent='This book is the best book on Python or M.L I have laid eyes upon.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This book is the best book on Python or M.L I have laid eyes upon.\n"
     ]
    }
   ],
   "source": [
    "print(mySent)\n",
    "listOfTokens =re.sub('[^A-Za-z]',' ',mySent).split(' ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['t',\n",
       " 'h',\n",
       " 'i',\n",
       " 's',\n",
       " ' ',\n",
       " 'b',\n",
       " 'o',\n",
       " 'o',\n",
       " 'k',\n",
       " ' ',\n",
       " 'i',\n",
       " 's',\n",
       " ' ',\n",
       " 't',\n",
       " 'h',\n",
       " 'e',\n",
       " ' ',\n",
       " 'b',\n",
       " 'e',\n",
       " 's',\n",
       " 't',\n",
       " ' ',\n",
       " 'b',\n",
       " 'o',\n",
       " 'o',\n",
       " 'k',\n",
       " ' ',\n",
       " 'o',\n",
       " 'n',\n",
       " ' ',\n",
       " 'p',\n",
       " 'y',\n",
       " 't',\n",
       " 'h',\n",
       " 'o',\n",
       " 'n',\n",
       " ' ',\n",
       " 'o',\n",
       " 'r',\n",
       " ' ',\n",
       " 'm',\n",
       " ' ',\n",
       " 'l',\n",
       " ' ',\n",
       " 'i',\n",
       " ' ',\n",
       " 'h',\n",
       " 'a',\n",
       " 'v',\n",
       " 'e',\n",
       " ' ',\n",
       " 'l',\n",
       " 'a',\n",
       " 'i',\n",
       " 'd',\n",
       " ' ',\n",
       " 'e',\n",
       " 'y',\n",
       " 'e',\n",
       " 's',\n",
       " ' ',\n",
       " 'u',\n",
       " 'p',\n",
       " 'o',\n",
       " 'n',\n",
       " ' ']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[tok.lower() for tok in listOfTokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "emailText = open('email/ham/6.txt').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello,\\n\\nSince you are an owner of at least one Google Groups group that uses the customized welcome message, pages or files, we are writing to inform you that we will no longer be supporting these features starting February 2011. We made this decision so that we can focus on improving the core functionalities of Google Groups -- mailing lists and forum discussions.  Instead of these features, we encourage you to use products that are designed specifically for file storage and page creation, such as Google Docs and Google Sites.\\n\\nFor example, you can easily create your pages on Google Sites and share the site (http://www.google.com/support/sites/bin/answer.py?hl=en&answer=174623) with the members of your group. You can also store your files on the site by attaching files to pages (http://www.google.com/support/sites/bin/answer.py?hl=en&answer=90563) on the site. If you抮e just looking for a place to upload your files so that your group members can download them, we suggest you try Google Docs. You can upload files (http://docs.google.com/support/bin/answer.py?hl=en&answer=50092) and share access with either a group (http://docs.google.com/support/bin/answer.py?hl=en&answer=66343) or an individual (http://docs.google.com/support/bin/answer.py?hl=en&answer=86152), assigning either edit or download only access to the files.\\n\\nyou have received this mandatory email service announcement to update you about important changes to Google Groups.'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emailText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
