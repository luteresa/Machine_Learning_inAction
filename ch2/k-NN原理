Ｋ－近邻算法原理(k-nearest neighbor,k-NN)：

输入是一个样本集，样本集每个数据都存在标签，输入待测试数据，求该数据对应标签分类；

将新数据的每个特征与样本集中数据对应特征进行比较，然后算法提取样本集中特征最相似的分类标签。

一般只选择数据集前k个最相似数据，k不大于20

2.k-NN算法优缺点：
优点
精度高、对异常值不敏感、无数据输入假定

缺点
计算复杂度高，空间复杂度高

3.适用数据范围：数值型和标称型

4.kNN算法特殊情况是当k=1时，成为最近邻算法。

5.k值的选择，距离度量，分类决策规则--->是kNN算法的三个基本要素
a.选较小的k值，相当于用较小的领域中的训练实例进行预测，“学习“的近似误差会减小，但”估计“误差会变大，预测结果会非常敏感，对噪声可能预测错误。
即k值的减小，意味着整体模型变得复杂，容易发生过拟合。

选较大的k值，优点是减少学习”估计“误差，但近似误差会变大。模型过于简单，就忽略了训练实例中的有用信息，是不可取的。

实际应用中，k值一般取一个比较小的数，通常采用交叉验证发来选择最佳k值。

b.距离度量：
特征空间中两个实例点的距离，是两个实例点相似程度的反映，kNN模型特征空间一般是n维的实数向量空间，使用欧式距离。当然也可以使用其他距离，比如Lp distance和Minkowski distance ;

c.分离决策规则
k-NN决策一般采用多数表决，即由k个邻近实例中的多数类决定输入的类；
出错率P(Y！=f(X))=1-P(Y=f(X))
多数表决规则等价于经验风险最小化。
